{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b40b0fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "from spacy.pipeline import merge_entities\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import PCA, NMF, TruncatedSVD, LatentDirichletAllocation\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import pairwise_distances\n",
    "\n",
    "from corextopic import corextopic as ct\n",
    "from corextopic import vis_topic as vt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f84a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('movie_summaries.csv', index_col=0)\n",
    "df['original_summary'] = df['summary']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4bf2858",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove digits from docs\n",
    "df['summary'] = df['summary'].str.replace('\\d+', ' ', regex=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2809ead8",
   "metadata": {},
   "source": [
    "# Baseline Topic Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e49d2c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_topics(docs, vectorizer, topic_modeler, n_words=15):\n",
    "    \n",
    "    # Vectorize documents into a document-word matrix.\n",
    "    doc_word_vectors = vectorizer.fit_transform(docs)\n",
    "    \n",
    "    # Fit the topic model.\n",
    "    doc_topic_vectors = topic_modeler.fit_transform(doc_word_vectors)\n",
    "    \n",
    "    # Print the topics.\n",
    "    vocab = vectorizer.get_feature_names()\n",
    "    for idx, topic in enumerate(topic_modeler.components_):\n",
    "        # Select the top 15 words in vocab for this topic.\n",
    "        top_words = [vocab[i].upper() for i in topic.argsort()[:-n_words-1:-1]]\n",
    "        print(f'Topic {idx}:\\n', ', '.join(top_words), '\\n')\n",
    "    \n",
    "    return doc_word_vectors, doc_topic_vectors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63cc99b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_formatter(token, doc_entities):\n",
    "    \n",
    "    token_text = token.lemma_.lower().strip().replace('star ', '')\n",
    "    \n",
    "    if 'academy award' in token.text.lower():\n",
    "        return 'academy award'\n",
    "        \n",
    "    elif token.text in doc_entities:\n",
    "        token_string = re.sub(f'[{string.punctuation}]+', ' ', token_text).strip().lower()\n",
    "        return re.sub(r'\\s+', ' ', token_string)\n",
    "        \n",
    "    elif token.pos_ == 'PROPN':\n",
    "        token_string = re.sub(f'[{string.punctuation}]+', ' ', token_text).strip().lower()\n",
    "        return re.sub(r'\\s+', ' ', token_string)\n",
    "    \n",
    "    else:\n",
    "        return token.lemma_.lower().strip()\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1819c4bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def spacy_tokenizer(spacy_doc):\n",
    "        \n",
    "    ent_list = ['DATE', 'CARDINAL', 'ORDINAL', 'MONEY']\n",
    "        \n",
    "    doc_entities = [ent.text for ent in spacy_doc.ents if (ent.label_ not in ent_list)]\n",
    "    ignore_ents = [ent.text for ent in spacy_doc.ents if (ent.label_ in ent_list)]\n",
    "    \n",
    "    # remove stop words, parts of speech, and punctuation\n",
    "    pos_list = ['SPACE', 'PUNCT', 'SYM']\n",
    "    puncs = string.punctuation.replace('+','_')\n",
    "\n",
    "    doc_tokens = [token for token in spacy_doc if (not token.is_stop) and (token.lemma_ not in nlp.Defaults.stop_words) and (token.pos_ not in pos_list) and (not any(p in token.text for p in puncs)) and (token.text not in ignore_ents)]\n",
    "\n",
    "    # lemmatize each token and convert to lowercase if POS is not a proper noun\n",
    "    doc_tokens = [token_formatter(token, doc_entities) for token in doc_tokens if len(token) > 2]\n",
    "\n",
    "    return doc_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c560b649",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_top_words(model, feature_names, n_words, title, rows=2, cols=5):\n",
    "\n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(30, 15), sharex=True)\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    if str(type(model)) == \"<class 'corextopic.corextopic.Corex'>\":\n",
    "        for topic_idx, topic_words in enumerate(model.get_topics(n_words=n_words)):\n",
    "            top_features = [word[0] for word in topic_words]\n",
    "            weights = [word[1] for word in topic_words]\n",
    "            \n",
    "            ax = axes[topic_idx]\n",
    "            ax.barh(top_features, weights, height=0.7)\n",
    "            ax.set_title(f'Topic {topic_idx +1}',\n",
    "                     fontdict={'fontsize': 30})\n",
    "            ax.invert_yaxis()\n",
    "            ax.tick_params(axis='both', which='major', labelsize=20)\n",
    "            for i in 'top right left'.split():\n",
    "                ax.spines[i].set_visible(False)\n",
    "    \n",
    "    else:\n",
    "        for topic_idx, topic in enumerate(model.components_):\n",
    "            top_features_ind = topic.argsort()[:-n_words - 1:-1]\n",
    "            top_features = [feature_names[i] for i in top_features_ind]\n",
    "            weights = topic[top_features_ind]\n",
    "\n",
    "            ax = axes[topic_idx]\n",
    "            ax.barh(top_features, weights, height=0.7)\n",
    "            ax.set_title(f'Topic {topic_idx +1}',\n",
    "                     fontdict={'fontsize': 30})\n",
    "            ax.invert_yaxis()\n",
    "            ax.tick_params(axis='both', which='major', labelsize=20)\n",
    "            for i in 'top right left'.split():\n",
    "                ax.spines[i].set_visible(False)\n",
    "    \n",
    "    fig.suptitle(title, fontsize=40)\n",
    "\n",
    "    plt.subplots_adjust(top=0.90, bottom=0.05, wspace=0.90, hspace=0.3)\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16fc22f2",
   "metadata": {},
   "source": [
    "## CountVectorizer, NMF, 10 topic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aea19cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = df['summary']\n",
    "\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "# add step to pipeline that merges named entities into a single token\n",
    "nlp.add_pipe('merge_entities')\n",
    "nlp.Defaults.stop_words |= {'min', 'release', 'film', 'video', 'location', 'include', 'direct', 'set', 'widescreen' ,'studio'}\n",
    "\n",
    "vectorizer = CountVectorizer(preprocessor=nlp, tokenizer=spacy_tokenizer)\n",
    "topic_modeler = NMF(10, max_iter=1000, random_state=42)\n",
    "\n",
    "doc_word_vectors, doc_topic_vectors = make_topics(docs, vectorizer, topic_modeler);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38281bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "plot_top_words(topic_modeler, vectorizer.get_feature_names(), 10, 'Topics in Count, NMF, 10 topic model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e187ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_topic_df = pd.DataFrame(doc_topic_vectors.round(5),\n",
    "                             index = df['title'])\n",
    "doc_topic_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7fff62d",
   "metadata": {},
   "source": [
    "## TfidfVectorizer, NMF, 10 topic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2fd8bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = df['summary']\n",
    "\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "# add step to pipeline that merges named entities into a single token\n",
    "nlp.add_pipe('merge_entities')\n",
    "nlp.Defaults.stop_words |= {'min', 'release', 'film', 'video', 'location', 'include', 'direct', 'set', 'widescreen' ,'studio'}\n",
    "\n",
    "vectorizer = TfidfVectorizer(preprocessor=nlp, tokenizer=spacy_tokenizer)\n",
    "topic_modeler = NMF(10, max_iter=1000, random_state=42)\n",
    "\n",
    "doc_word_vectors, doc_topic_vectors = make_topics(docs, vectorizer, topic_modeler);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caeb7404",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "plot_top_words(topic_modeler, vectorizer.get_feature_names(), 10, 'Topics in Tfidf, NMF, 10 topic model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19777fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_topic_df = pd.DataFrame(doc_topic_vectors.round(5),\n",
    "                             index = df['title'])\n",
    "doc_topic_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df14098e",
   "metadata": {},
   "source": [
    "## CountVectorizer, CorEx, 10 topic model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af9417d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = df['summary']\n",
    "\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "# add step to pipeline that merges named entities into a single token\n",
    "nlp.add_pipe('merge_entities')\n",
    "nlp.Defaults.stop_words |= {'min', 'release', 'film', 'video', 'location', 'include', 'direct', 'set', 'widescreen' ,'studio'}\n",
    "\n",
    "vectorizer = CountVectorizer(preprocessor=nlp, tokenizer=spacy_tokenizer, binary=True)\n",
    "\n",
    "doc_word_vectors = vectorizer.fit_transform(docs)\n",
    "words = list(np.asarray(vectorizer.get_feature_names()))\n",
    "\n",
    "topic_modeler = ct.Corex(n_hidden=10, words=words, seed=42)\n",
    "topic_modeler.fit(doc_word_vectors, words=words, docs=docs)\n",
    "\n",
    "topics = topic_modeler.get_topics()\n",
    "for n,topic in enumerate(topics):\n",
    "    topic_words,_,_ = zip(*topic)\n",
    "    print('Topic {}:'.format(n))\n",
    "    print(', '.join(topic_words), '\\n')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c864b149",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "plot_top_words(topic_modeler, vectorizer.get_feature_names(), 10, 'Topics in Count, Corex, 10 topic model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4284ad63",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "plt.bar(range(topic_modeler.tcs.shape[0]), topic_modeler.tcs, width=0.5)\n",
    "plt.xlabel('Topic', fontsize=16)\n",
    "plt.ylabel('Total Correlation (nats)', fontsize=16)\n",
    "plt.show();\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c24ccb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_topic_df = pd.DataFrame(topic_modeler.p_y_given_x,\n",
    "                             index = df['title'])\n",
    "doc_topic_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c8d6749",
   "metadata": {},
   "source": [
    "## TfidfVectorizer, TruncatedSVD, 10 topic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0927c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = df['summary']\n",
    "\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "# add step to pipeline that merges named entities into a single token\n",
    "nlp.add_pipe('merge_entities')\n",
    "nlp.Defaults.stop_words |= {'min', 'release', 'film', 'video', 'location', 'include', 'direct', 'set', 'widescreen' ,'studio'}\n",
    "\n",
    "vectorizer = TfidfVectorizer(preprocessor=nlp, tokenizer=spacy_tokenizer)\n",
    "topic_modeler = TruncatedSVD(n_components=10, random_state=42)\n",
    "\n",
    "doc_word_vectors, doc_topic_vectors = make_topics(docs, vectorizer, topic_modeler);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb213cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "plot_top_words(topic_modeler, vectorizer.get_feature_names(), 10, 'Topics in Tfidf, SVD, 10 topic model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c7d635a",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_topic_df = pd.DataFrame(doc_topic_vectors.round(5),\n",
    "                             index = df['title'])\n",
    "doc_topic_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb90f1b",
   "metadata": {},
   "source": [
    "## CountVectorizer, CorEx, 10 topic model, tuning min_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "787711db",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = df['summary']\n",
    "\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "# add step to pipeline that merges named entities into a single token\n",
    "nlp.add_pipe('merge_entities')\n",
    "nlp.Defaults.stop_words |= {'min', 'release', 'film', 'video', 'location', 'include', 'direct', 'set', 'widescreen', 'studio'}\n",
    "\n",
    "vectorizer = CountVectorizer(preprocessor=nlp, tokenizer=spacy_tokenizer, min_df=0.0025, binary=True)\n",
    "\n",
    "doc_word_vectors = vectorizer.fit_transform(docs)\n",
    "words = list(np.asarray(vectorizer.get_feature_names()))\n",
    "\n",
    "topic_modeler = ct.Corex(n_hidden=10, words=words, seed=42)\n",
    "topic_modeler.fit(doc_word_vectors, words=words, docs=docs)\n",
    "\n",
    "topics = topic_modeler.get_topics()\n",
    "for n,topic in enumerate(topics):\n",
    "    topic_words,_,_ = zip(*topic)\n",
    "    print('Topic {}:'.format(n))\n",
    "    print(', '.join(topic_words), '\\n')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "625d41e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "plot_top_words(topic_modeler, vectorizer.get_feature_names(), 10, 'Topics in Count, Corex, 10 topic model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3caa145d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "plt.bar(range(topic_modeler.tcs.shape[0]), topic_modeler.tcs, width=0.5)\n",
    "plt.xlabel('Topic', fontsize=16)\n",
    "plt.ylabel('Total Correlation (nats)', fontsize=16)\n",
    "plt.show();\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50367e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_topic_df = pd.DataFrame(topic_modeler.p_y_given_x,\n",
    "                             index = df['title'])\n",
    "doc_topic_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76f13151",
   "metadata": {},
   "source": [
    "## CountVectorizer, CorEx, tuning topics and anchor topics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36bc3db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = df['summary']\n",
    "\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "# add step to pipeline that merges named entities into a single token\n",
    "nlp.add_pipe('merge_entities')\n",
    "nlp.Defaults.stop_words |= {'min', 'release', 'film', 'video', 'movie', 'location', 'include', 'direct', 'set', 'widescreen', 'studio', 'cartoon', 'touchstone', 'cinemascope'}#,  'touchstone pictures', 'caravan pictures',  'hollywood pictures', 'hollywood picture'}#'educational'}\n",
    "\n",
    "vectorizer = CountVectorizer(preprocessor=nlp, tokenizer=spacy_tokenizer, min_df=0.0025, binary=True)\n",
    "\n",
    "doc_word_vectors = vectorizer.fit_transform(docs)\n",
    "words = list(np.asarray(vectorizer.get_feature_names()))\n",
    "\n",
    "topic_anchors = [\n",
    "                 ['mickey', 'mickey mouse', 'pluto'],\n",
    "                 ['donald', 'donald duck', 'nephew'],\n",
    "                 ['live', 'action'],\n",
    "                 ['academy', 'award'],\n",
    "                 ['pooh', 'tigger', 'piglet'],\n",
    "                 #['book', 'novel'],\n",
    "                 ['educational']\n",
    "                ]\n",
    "\n",
    "topic_modeler = ct.Corex(n_hidden=27, words=words, seed=42,\n",
    "                         anchors=topic_anchors, anchor_strength=3)\n",
    "topic_modeler.fit(doc_word_vectors, words=words, docs=docs)\n",
    "\n",
    "topics = topic_modeler.get_topics()\n",
    "for n,topic in enumerate(topics):\n",
    "    topic_words,_,_ = zip(*topic)\n",
    "    print('Topic {}:'.format(n+1))\n",
    "    print(', '.join(topic_words), '\\n')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0bcdc9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "plt.bar(range(topic_modeler.tcs.shape[0]), topic_modeler.tcs, width=0.5)\n",
    "plt.xlabel('Topic', fontsize=16)\n",
    "plt.ylabel('Total Correlation (nats)', fontsize=16)\n",
    "plt.show();\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72d5ce3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "plot_top_words(topic_modeler, vectorizer.get_feature_names(), 10, 'Topics in Count, Corex model', rows=3, cols=9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f9052b",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_topic_df = pd.DataFrame(topic_modeler.labels,\n",
    "                            index = df['title'], \n",
    "                            columns=[f'topic_{n}' for n in range(1,27+1)])\n",
    "doc_topic_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe1d06cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_topic_df = pd.DataFrame(topic_modeler.p_y_given_x,\n",
    "                            #index = df['title'], \n",
    "                            columns=[f'topic_{n}' for n in range(1,27+1)])\n",
    "doc_topic_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd176457",
   "metadata": {},
   "source": [
    "## Test recommendation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e056d8cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_distance_metrics(movie_idx, n_recs=3):\n",
    "    print(df['title'][movie_idx])\n",
    "    print(df['original_summary'][movie_idx][:100] + '...')\n",
    "    print('\\n')\n",
    "    \n",
    "    distance_metrics = ['cosine', 'euclidean', 'l1', 'l2']\n",
    "    \n",
    "    for metric in distance_metrics:\n",
    "        recs = pairwise_distances(np.array(doc_topic_df.iloc[movie_idx]).reshape(1,-1), doc_topic_df, metric=metric).argsort()[0][1:]\n",
    "        print(f'Recommendations using {metric} distance metric:')\n",
    "        \n",
    "        for i in range(n_recs):\n",
    "            print('\\t', df['title'][recs[i]])\n",
    "            #print('\\t', df['original_summary'][recs[i]][:100] + '...')\n",
    "        print('\\n')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b93acca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_distance_metrics(211)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b64bdea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_distance_metrics(1095)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9784fb24",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_distance_metrics(1762)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f1fd561",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_distance_metrics(2095)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f12ae753",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_distance_metrics(260)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "749713f4",
   "metadata": {},
   "source": [
    "## Test single recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6213d63d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommendation_single(movie_idx, n_recs=5):\n",
    "    print(df['title'][movie_idx])\n",
    "    print(df['original_summary'][movie_idx][:100] + '...')\n",
    "    print('\\n')\n",
    "        \n",
    "    recs = pairwise_distances(np.array(doc_topic_df.iloc[movie_idx]).reshape(1,-1), doc_topic_df, metric='cosine').argsort()[0][1:]\n",
    "        \n",
    "    for i in range(n_recs):\n",
    "        print('\\t', df['title'][recs[i]])\n",
    "        print('\\t', df['original_summary'][recs[i]][:100] + '...')\n",
    "        print('\\n')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d68a737d",
   "metadata": {},
   "outputs": [],
   "source": [
    "recommendation_single(210, n_recs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "517aa91a",
   "metadata": {},
   "outputs": [],
   "source": [
    "recommendation_single(1762, n_recs=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c096745",
   "metadata": {},
   "source": [
    "## Test pair recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d1f8e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommendation_pair(movie_idx_1, movie_idx_2, n_recs=5):\n",
    "    print('First selected movie:')\n",
    "    print(df['title'][movie_idx_1])\n",
    "    print(df['original_summary'][movie_idx_1][:100] + '...')\n",
    "    print('\\n')\n",
    "    \n",
    "    print('Second selected movie:')\n",
    "    print(df['title'][movie_idx_2])\n",
    "    print(df['original_summary'][movie_idx_2][:100] + '...')\n",
    "    print('\\n')\n",
    "    \n",
    "    print('Getting recommendations...')\n",
    "    \n",
    "    recs_1 = pairwise_distances(np.array(doc_topic_df.iloc[movie_idx_1]).reshape(1,-1), doc_topic_df, metric='cosine').argsort()[0]\n",
    "    recs_2 = pairwise_distances(np.array(doc_topic_df.iloc[movie_idx_2]).reshape(1,-1), doc_topic_df, metric='cosine').argsort()[0]\n",
    "\n",
    "    rec_ranks = []\n",
    "    for i,title in enumerate(df['title']):\n",
    "        rec_ranks.append(list(recs_1).index(i) + list(recs_2).index(i)) \n",
    "        \n",
    "    rec_idx = np.argpartition(rec_ranks, n_recs)\n",
    "    \n",
    "    print('\\n')\n",
    "    if (movie_idx_1 in rec_idx[:n_recs]):\n",
    "        n_recs += 1\n",
    "    if (movie_idx_2 in rec_idx[:n_recs]):\n",
    "        n_recs += 1\n",
    "    if (movie_idx_1 == movie_idx_2):\n",
    "        n_recs -= 1\n",
    "        \n",
    "    for i in rec_idx[:n_recs]:\n",
    "        if i not in [movie_idx_1, movie_idx_2]:\n",
    "            print('\\t', df['title'][i])\n",
    "            print('\\t', df['original_summary'][i][:100] + '...')\n",
    "            print('\\n')\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0613015e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "recommendation_pair(210, 1762, n_recs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d91b1051",
   "metadata": {},
   "outputs": [],
   "source": [
    "recommendation_pair(210, 1522, n_recs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2124a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Walt Disney's favorite movies: Bambi, Dumbo\n",
    "recommendation_pair(184, 584, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec6b928e",
   "metadata": {},
   "outputs": [],
   "source": [
    "recommendation_pair(1896, 1981, n_recs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c06ec7dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_topic_df.to_pickle('doc_topic_df.pkl')\n",
    "df.to_pickle('movie_summaries.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bb4e689",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:metis] *",
   "language": "python",
   "name": "conda-env-metis-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
